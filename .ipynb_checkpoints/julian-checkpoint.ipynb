{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# location of original images\n",
    "subdirectory = 'images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  imageName       ylabel  plane helicopter  other aircraft\n",
      "0  2016-10-23T10+40+20_430Z  No aircraft  False      False  False    False\n",
      "1  2016-11-26T20+15+55_250Z  No aircraft  False      False  False    False\n",
      "2  2016-12-15T15+26+33_480Z  No aircraft  False      False  False    False\n",
      "3  2016-10-23T14+01+49_396Z  No aircraft  False      False  False    False\n",
      "4  2016-08-14T05+43+34_690Z  No aircraft  False      False  False    False\n",
      "(2410, 6)\n"
     ]
    }
   ],
   "source": [
    "# read labels for aircraft images\n",
    "labels = pd.read_csv('aircraft.csv')\n",
    "print(labels.head())\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage import io, color\n",
    "\n",
    "# iterate through the aircraft.csv file to get image names and labels\n",
    "# read the images, convert to grayscale, and store image ndarrays and labels in lists\n",
    "grayscale_images_list = []\n",
    "y_list = []\n",
    "\n",
    "for index, row in labels.iterrows():\n",
    "    toRead = subdirectory + row['imageName']\n",
    "    grayscale_images_list.append(color.rgb2gray(io.imread(toRead)))\n",
    "    y_list.append(row['aircraft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2410, 360, 640)\n"
     ]
    }
   ],
   "source": [
    "# convert the lists to ndarrays\n",
    "grayscale_images = np.asarray(grayscale_images_list)\n",
    "Y = np.asarray(y_list)\n",
    "print(grayscale_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2410, 230400)\n",
      "(2410,)\n"
     ]
    }
   ],
   "source": [
    "# flatten the images ndarray to one row per image\n",
    "imgs_flat = grayscale_images.reshape((grayscale_images.shape[0], -1))\n",
    "print(imgs_flat.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready Data for Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aaron Hill's Binary Classification Evaluator\n",
    "class BinaryClassificationPerformance():\n",
    "    '''Performance measures to evaluate the fit of a binary classification model'''\n",
    "    \n",
    "    def __init__(self, predictions, labels, desc, probabilities=None):\n",
    "        '''Initialize attributes: predictions-vector of predicted values for Y, labels-vector of labels for Y'''\n",
    "        '''probabilities-optional, probability that Y is equal to True'''\n",
    "        self.probabilities = probabilities\n",
    "        self.performance_df = pd.concat([pd.DataFrame(predictions), pd.DataFrame(labels)], axis=1)\n",
    "        self.performance_df.columns = ['preds', 'labls']\n",
    "        self.desc = desc\n",
    "        self.performance_measures = {}\n",
    "  \n",
    "    def compute_measures(self):\n",
    "        '''Compute performance measures defined by Flach p. 57'''\n",
    "        self.performance_measures['Pos'] = self.performance_df['preds'].sum()\n",
    "        self.performance_measures['Neg'] = self.performance_df.shape[0] - self.performance_df['preds'].sum()\n",
    "        self.performance_measures['TP'] = ((self.performance_df['preds'] == True) & (self.performance_df['labls'] == True)).sum()\n",
    "        self.performance_measures['TN'] = ((self.performance_df['preds'] == False) & (self.performance_df['labls'] == False)).sum()\n",
    "        self.performance_measures['FP'] = ((self.performance_df['preds'] == True) & (self.performance_df['labls'] == False)).sum()\n",
    "        self.performance_measures['FN'] = ((self.performance_df['preds'] == False) & (self.performance_df['labls'] == True)).sum()\n",
    "        self.performance_measures['Accuracy'] = (self.performance_measures['TP'] + self.performance_measures['TN']) / (self.performance_measures['Pos'] + self.performance_measures['Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Separate out train and test sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(imgs_flat, Y,\n",
    "                                                          test_size = 0.5,\n",
    "                                                          random_state = 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allow pickle creation\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.97759336099585059, 'FP': 0, 'TP': 0, 'Pos': 0, 'TN': 1178, 'Neg': 1205, 'FN': 27}\n"
     ]
    }
   ],
   "source": [
    "#Try SVM\n",
    "from sklearn import linear_model\n",
    "svm = linear_model.SGDClassifier()\n",
    "svm.fit(data_train, y_train)\n",
    "joblib.dump(svm, 'svm.pkl') # pickle\n",
    "\n",
    "svm_performance = BinaryClassificationPerformance(svm.predict(data_train), y_train, 'svm')\n",
    "svm_performance.compute_measures()\n",
    "print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try logistic model\n",
    "from sklearn import linear_model\n",
    "lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "lgs.fit(data_train, y_train)\n",
    "joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "\n",
    "lgs_performance = BinaryClassificationPerformance(lgs.predict(data_train), y_train, 'lgs')\n",
    "lgs_performance.compute_measures()\n",
    "print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nbs = MultinomialNB()\n",
    "nbs.fit(data_train, y_train)\n",
    "joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "\n",
    "nbs_performance = BinaryClassificationPerformance(nbs.predict(data_train), y_train, 'nbs')\n",
    "nbs_performance.compute_measures()\n",
    "print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try Ridge Regression Classifier\n",
    "from sklearn import linear_model\n",
    "rdg = linear_model.RidgeClassifier()\n",
    "rdg.fit(data_train, y_train)\n",
    "joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "\n",
    "rdg_performance = BinaryClassificationPerformance(rdg.predict(data_train), y_train, 'rdg')\n",
    "rdg_performance.compute_measures()\n",
    "print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Perceptron\n",
    "from sklearn import linear_model\n",
    "prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "prc.fit(data_train, y_train)\n",
    "joblib.dump(prc, 'prc.pkl') # pickle\n",
    "\n",
    "prc_performance = BinaryClassificationPerformance(prc.predict(data_train), y_train, 'prc')\n",
    "prc_performance.compute_measures()\n",
    "print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check Model Performance of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fits[1].performance_measures['TP'] / fit.performance_measures['Pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'ro')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: training set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try SVM\n",
    "svm = joblib.load('svm.pkl')\n",
    "\n",
    "svm_performance = BinaryClassificationPerformance(svm.predict(data_train), y_train, 'svm')\n",
    "svm_performance.compute_measures()\n",
    "print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try logistic model\n",
    "lgs = joblib.load('lgs.pkl')\n",
    "\n",
    "lgs_performance = BinaryClassificationPerformance(lgs.predict(data_train), y_train, 'lgs')\n",
    "lgs_performance.compute_measures()\n",
    "print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try Naive Bayes\n",
    "nbs = joblib.load('nbs.pkl')\n",
    "\n",
    "nbs_performance = BinaryClassificationPerformance(nbs.predict(data_train), y_train, 'nbs')\n",
    "nbs_performance.compute_measures()\n",
    "print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try Ridge Regression Classifier\n",
    "rdg = joblib.load('rdg.pkl')\n",
    "\n",
    "rdg_performance = BinaryClassificationPerformance(rdg.predict(data_train), y_train, 'rdg')\n",
    "rdg_performance.compute_measures()\n",
    "print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try Perceptron\n",
    "prc = joblib.load('prc.pkl')\n",
    "\n",
    "prc_performance = BinaryClassificationPerformance(prc.predict(data_train), y_train, 'prc')\n",
    "prc_performance.compute_measures()\n",
    "print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check Model Performance of Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'ro')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: training set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
